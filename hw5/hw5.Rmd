---
title: 'Topic 5: Word Relationships'
author: "Mia Forsline"
date: '2022-05-03'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE)

#install packages if necessary, then load libraries
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

librarian::shelf(
  forcats,
  ggraph,
  here,
  igraph,
  kableExtra,
  lubridate,
  pdftools,
  quanteda,
  quanteda.sentiment,
  quanteda.textplots,
  quanteda.textstats,
  readr,
  readtext,
  reshape2,
  sentimentr,
  stringr,
  tidyr,
  tidytext,
  tidyverse,
  widyr,
  wordcloud
  )
```

Import EPA environmental justice data

```{r}
#character of full file paths for each PDF in the data folder 
files <- list.files(path = here::here("data"),
                    pattern = "pdf$", full.names = TRUE)

#subset to include only EPA report PDFs 
files <- str_subset(files, pattern="EPA")

#list
ej_reports <- lapply(files, pdf_text)

#readtext/df of all 6 PDf reports 
ej_pdf <- readtext(file = files,
                   docvarsfrom = "filenames",
                   docvarnames = c("type","year"),
                   sep = "_")

#creating an initial corpus containing the data
epa_corp <- corpus(x = ej_pdf, text_field = "text" )

#check that the corpus contains all 6 reports 
summary(epa_corp)
```

Add additional, context-specific stop words to stop word lexicon
```{r}
more_stops <-c("2015",
               "2016", 
               "2017", 
               "2018", 
               "2019", 
               "2020", 
               "www.epa.gov", 
               "https")
add_stops <- tibble(word = c(stop_words$word, more_stops)) 
stop_vec <- as_vector(add_stops)
```

Tokenize the data into single words 
```{r}
tokens <- tokens(epa_corp, remove_punct = TRUE)

toks1<- tokens_select(tokens, min_nchar = 3)
toks1 <- tokens_tolower(toks1) #lowercase 
toks1 <- tokens_remove(toks1, pattern = (stop_vec)) #remove stop words 
dfm <- dfm(toks1) #create a data frequency matrix 
```

# 1. What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?

Calculate the 10 most frequent bigrams 
```{r bigrams}
toks2 <- tokens_ngrams(toks1, n=2)
dfm2 <- dfm(toks2)
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n=20)
freq_words2$token <- rep("bigram", 20)

tstat_freq2 <- textstat_frequency(dfm2, n = 5, groups = year)

#display the top 10 rows in a table 
tstat_freq2[1:10] %>% 
  rename(Bigram = feature,
         "Report Year" = group,
         Frequency = frequency,
         Rank = rank,
         "Document Frequency" = docfreq) %>% 
  kbl() %>% 
  kable_styling()
  
```
\newpage 

Calculate the most frequent trigrams 
```{r trigrams}
toks3 <- tokens_ngrams(toks1, n=3)
dfm3 <- dfm(toks3)
dfm3 <- dfm_remove(dfm3, pattern = c(stop_vec))
freq_words3 <- textstat_frequency(dfm3, n=20)
freq_words3$token <- rep("trigram", 20)

tstat_freq3 <- textstat_frequency(dfm3, n = 5, groups = year)

#display the top 10 rows in a table 
tstat_freq3[1:10] %>% 
  rename(Trigram = feature,
         "Report Year" = group,
         Frequency = frequency,
         Rank = rank,
         "Document Frequency" = docfreq) %>% 
  kbl() %>% 
  kable_styling()
```
After comparing the 10 most common bigrams and trigrams, the additional third word does not seem to add much more meaning to the results. For example, "environmental_justice" is just as informative as "national_environmental_justice." The third word can even be more confusing. For instance "page_fiscal_annual" is an unclear trigram. Thus, the bigrams seem more informative to me. 

# 2. Choose a new focal term to replace “justice” and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!

Put the text into tidy format and extract individual words from each paragraph 

```{r}
#convert to tidy format
raw_text <- tidy(epa_corp)

#paragraph tokens (tibble)
par_tokens <- unnest_tokens(raw_text, output = paragraphs, input = text, token = "paragraphs")

#give each paragraph an id number 
par_tokens <- par_tokens %>%
 mutate(par_id = 1:n())

#extract individual words from each paragraph 
par_words <- unnest_tokens(par_tokens, output = word, input = paragraphs, token = "words")

```

Identify which words tend to occur closely together in the text 

```{r}
word_pairs <- par_words %>% 
  pairwise_count(word, par_id, sort = TRUE, upper = FALSE) %>%
  anti_join(add_stops, by = c("item1" = "word")) %>%
  anti_join(add_stops, by = c("item2" = "word"))

#calculate correlation coefficients between words 
word_cors <- par_words %>% 
  add_count(par_id) %>% 
  filter(n >= 50) %>% 
  select(-n) %>%
  pairwise_cor(word, par_id, sort = TRUE)
```

Search for and graph words correlated with the term "urban"

```{r}
#search for words correlated with "urban" (tibble)
urban_cors <- word_cors %>% 
  filter(item1 == "urban")

#create facet_wrap labels
supp.labs <- c("Waters", 
               "Flooding", 
               "Housing",
               "Urban")
names(supp.labs) <- c("waters", 
                      "flooding",
                      "housing",
                      "urban")

#graph correlation coefficients for words correlated with 4 key terms based on the correlation coefficients calculated in urban_cors 
word_cors %>%
  filter(item1 %in% c("waters", 
                      "flooding", 
                      "housing", 
                      "urban")) %>%
  group_by(item1) %>%
  top_n(5) %>% #display top 5 terms 
  ungroup() %>%
  mutate(item1 = as.factor(item1),
  name = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(y = name, x = correlation, fill = item1)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~item1, 
             ncol = 2, 
             scales = "free",
             labeller = labeller(item1 = supp.labs))+
  scale_y_reordered() +
  labs(y = NULL,
         x = NULL,
         title = "Word correlation strength with 4 key words",
         subtitle = "Data taken from 2015 - 2020 EPA EJ Reports") + 
  palettetown::scale_fill_poke(pokemon = "Beedrill", spread = 4) #select a Pokemon themed color palette from the 'palettetown' package 
```

Zoom in on just the key term "urban" and the 50 most related words 

```{r}
urban_cors <- word_cors %>% 
  filter(item1 == "urban") %>%
  mutate(n = 1:n())
urban_cors  %>%
  filter(n <= 50) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, 
                     edge_width = correlation), 
                 edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

# 3. Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.

```{r}
keyness <- textstat_keyness(dfm, target = 1) #target = 1 --> first document in the list, so in this case 2015
#target = 2 would use 2016 as the target document
textplot_keyness(keyness)

#chi-square values for each word 
#in 2015, climate has the highest chi-square value, so climate occurred the most "more than expected" given the baseline frequency 
#words in blue = occured more than expected
#words in grey = occurred less than expected 
#reference = includes all reports except 2015 
```

```{r}


keyness_function <- function(report_number) {
  
  for (i in (1:(length(epa_corp) -1))) {
    report_test <- epa_corp[i: (i+1)]
    print(report_test)
    tokens_test <- tokens(report_test, remove_punct = TRUE) #list split word by word
    tokens_test <- tokens_select(tokens_test, min_nchar = 3)
    tokens_test <- tokens_tolower(tokens_test)
    tokens_test <- tokens_remove(tokens_test, pattern = (stop_vec))
    dfm_test <- dfm(tokens_test) #document feature matrix

    keyness <- textstat_keyness(dfm_test, target = 1)
    print(textplot_keyness(keyness))
    
  }
  
}

keyness_function(report_number = 1)



```



# 4. Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create to objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? Hint

