---
title: "2_text_data_in_r"
author: "Mia Forsline"
date: "4/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Connect to the [New York Times Article Search API](https://developer.nytimes.com/) and send a query 
- we will be examining articles about Deb Haaland, the current US Secretary of the Interior 
- to do so, we will query the term "haaland" 
```{r}
library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse) 
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates
```


```{r}
#create an object called x with the results of our query ("haaland")
# the from JSON flatten the JSON object, then convert to a data frame

key = "zwtPmtpUJiMGPfGjXz8maQyr3cx6YFdS"

t <- fromJSON("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=zwtPmtpUJiMGPfGjXz8maQyr3cx6YFdS", flatten = TRUE) #the string following "key=" is your API key 

#q=haaland --> query 
#api-key --> credentials to log into the API 
#fromJSON takes the URL, queries the API, then flattens the data 

class(t) #what type of object is t? t is a list 

t <- t %>% 
  data.frame() #convert t into a data.frame 


#Inspect our data
class(t) #now what is it? t is dataframe
dim(t) # how big is it? 10 x 33 = 10 results/articles from NYT + 33 fields that comes along with each article object 
names(t) # what variables are we working with?
#t <- readRDS("nytDat.rds") #in case of API emergency :)
```

The name format, response.xxx.xxx…, is a legacy of the json nested hierarchy.

Let’s look at a piece of text. Our data object has a variable called “response.docs.snippet” that contains a short excerpt, or “snippet” from the article. Let’s grab a snippet and try out some basic ‘stringr’ functions.

```{r}
t$response.docs.snippet[9] #pull out an interesting sentence 

#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.

x <- "Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance." 

tolower(x) #all lower case --> uniform formatting makes sure we don't think Her vs her is different 
str_split(x, ','); #split the string at comma
str_split(x, 't') #split the string at the letter t 
str_replace(x, 'historic', 'without precedent') #replace "historic" with "without precedent" 
str_replace(x, ' ', '_') #replaces only the first space with an understoce
#how do we replace all of them?
str_replace_all(x, ' ', '_')

str_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F
str_locate(x, 't'); str_locate_all(x, 'as')
```

OK, it’s working but we want more data. Let’s set some parameters for a bigger query

```{r}
term <- "Haaland" # Need to use + to string together separate words
begin_date <- "20210120" #start our search at the beginning of the nomination process
end_date <- "20220401" #end our search at the end of the nomination process 

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&begin_date=",begin_date,
                  "&end_date=",end_date,
                  "&facet_filter=true&api-key=","zwtPmtpUJiMGPfGjXz8maQyr3cx6YFdS", sep="")

#examine our query url
baseurl
```

Next, we can query the API using our `baseurl`

```{r}
#this code allows for obtaining multiple pages of query results 
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 

#loop through maxPages so we can loop through all the articles 
pages <- list()
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(1) 
}
class(nytSearch)

#need to bind the pages and create a tibble from nytDa
```

```{r}
#nytDat <- read.csv("nytDat.csv") # obtained from 

nytSearch %>% 
  group_by(response.docs.type_of_material) %>%
  summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()
```

```{r}
nytSearch %>%
  mutate(pubDay=gsub(pattern = "T.*",
                     replacement = "", 
                     x = response.docs.pub_date)) %>%
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  #filter(count >= 2) %>% #filter for multiple articles per day 
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") + coord_flip()
```

```{r}
paragraph <- names(nytSearch)[6] #The 6th column, "response.doc.lead_paragraph", is the one we want here.  

tokenized <- nytSearch %>%
  unnest_tokens(word, paragraph)
#we're taking in paragraphs, un-nesting those paragraphs to the word level 

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>% #illegible with all the words displayed, so we can filter for only words that occur at least n times 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)

#however, many stopwords come up, so we should remove them 
```
Remove common stopwords 

```{r}
data(stop_words)
stop_words

tokenized <- tokenized %>%
  anti_join(stop_words) #anti-join to remove stopwords from our dataframe

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 2) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

OK, but look at the most common words. Does one stick out?

```{r}
#inspect the list of tokens (words)
tokenized$word

clean_tokens <- str_replace_all(tokenized$word,"land[a-z,A-Z]*","land") #stem tribe words
clean_tokens <- str_remove_all(clean_tokens, "[:digit:]") #remove all numbers 
#[:digit:] = find and select all digits 
#we can now see there are empty cells 

#clean_tokens <- str_remove_all(clean_tokens, "washington")

clean_tokens <- gsub("’s", '', clean_tokens) #base R function: global substitution 
#for example, we don't care about the difference between "Biden" and "Biden's" 

tokenized$clean <- clean_tokens

tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 10) %>% #illegible with all the words displayed
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)

#remove the empty strings
tib <-subset(tokenized, clean!="")

#reassign
tokenized <- tib

#try again
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 10) %>% #illegible with all the words displayed
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```

