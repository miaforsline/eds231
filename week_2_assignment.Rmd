---
title: "Topic 2: Text Data in R Assignment"
author: "Mia Forsline"
date: "4/6/2022"
output: html_document
---

# Set up: Load necessary packages 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warnings = FALSE,
                      messages = FALSE)

library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse) 
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates
```

# 2. Connect to the New York Times API and send a query

- Query phrase: "bark beetle" 

```{r, eval = FALSE}
#create an object called x with the results of our query ("bark beetle")
# the from JSON flatten the JSON object, then convert to a data frame

key = "zwtPmtpUJiMGPfGjXz8maQyr3cx6YFdS"

x <- fromJSON("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=bark+beetle&api-key=zwtPmtpUJiMGPfGjXz8maQyr3cx6YFdS", 
              flatten = TRUE) #the string following "key=" is your API key

#convert x to a dataframe 
x <- x %>% 
  data.frame()
```

## Examine a piece of text

- the data object has a variable called "response.docs.snippet" that contains a short excerpt, or "snippet" from the article. 
- check the snippet to ensure that the query picked out articles containing the correct query word ("bark beetle")

```{r, eval=FALSE}
x$response.docs.snippet[9]
```

## Set some parameters for a bigger query

- searching for articles from 1990 - 2022 

```{r}
term <- "bark+beetle" # Need to use + to string together separate words
begin_date <- "19900101" #start date: Jan 1, 1990
end_date <- "20220101" #end date: Jan 1, 2022
key = "zwtPmtpUJiMGPfGjXz8maQyr3cx6YFdS"

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&begin_date=",begin_date,
                  "&end_date=",end_date,
                  "&facet_filter=true&api-key=",key, sep="")

#examine our query url
baseurl
```

Obtain multiple pages of query results 

- retrieved 35 pages of results 
- retrieved 358 articles 

```{r, eval=FALSE}
initialQuery <- fromJSON(baseurl)

maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 

pages <- list()

for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(6) 
}

class(nytSearch)
#need to bind the pages and create a tibble from nytDat

nytDat <- rbind_pages(pages)
```

# 3. Visuzalize publications per day 

```{r}
nytDat %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>%
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") + coord_flip() + 
  labs(y = "Publication Day", 
       x = "Number of Articles Published")
```

The New York Times doesn't make full text of the articles available through the API. But we can use the first paragraph of each article.

- unnest the words from the first paragraph of each article 
```{r}
#names(nytDat)

paragraph <- names(nytDat)[6] #The 6th column, "response.doc.lead_paragraph", is the one we want here.  

tokenized <- nytDat %>%
  unnest_tokens(word, paragraph)

#tokenized[,34] 
```

# 3. Visualize word frequency plotsusing the first paragraph of each article 
- note that the most common words (occurring more than 100 times) are not at all meaningful 

```{r}
tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>% #illegible with all the words displayed
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = "Most common words in first paragraph", 
       x = "Word Frequency")
```

## Since this visual is not very helpful, we transform the corpus in various ways to help filter out less meaningful words from our visual 

1. incorporate stop words
2. stem tribe words
3. remove numbers 
4. remove possessive words 

```{r}

#call common stop words from a lexicon 
data(stop_words)
stop_words

#use an anti-join to remove the stop words from our tokenized object 
tokenized <- tokenized %>%
  anti_join(stop_words)

#inspect the list of tokens (words)
#tokenized$word

clean_tokens <- str_replace_all(tokenized$word,"fringe[a-z,A-Z]*","fringe")

#check to see if the transformation worked 
# %>% 
#   as.data.frame()
# clean_tokens$.

clean_tokens <- str_remove_all(clean_tokens, "[:digit:]") 

#check to see if the transformation worked 
# %>% 
#   as.data.frame()
# clean_tokens$.

#clean_tokens <- gsub("’s", '', clean_tokens) 

clean_tokens <- gsub("[^A-z]s", "", clean_tokens) 

# %>% 
#   as.data.frame()
# clean_tokens$.

#Put clean tokens into the `tokenized` dataframe 

tokenized$clean <- clean_tokens
```

## Visualize word frequency after transforming/cleaning

- remove the empty strings 
- limit the graph to only include words that occur over 20 times 

```{r, eval=FALSE}
#remove the empty strings
tib <-subset(tokenized, clean!="")
tib <- subset(tib, clean!=",")
  
#reassign
tokenized <- tib

#visualize again
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 20) %>% 
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = "Most common words in first paragraphs", 
       x = "Word Occurrence Frequency") + 
  theme_classic()

# ggsave(filename = "words_par.jpg",
#        width = 4,
#        height = 3,
#        units = c("in"),
#        dpi = 300)
```

# 4. Visualize publications per day ? 

# 4. Visualize word frequency using headlines 

```{r}
headline <- names(nytDat)[21] #The 6th column, "response.doc.lead_paragraph", is the one we want here.  

tokenized_h <- nytDat %>%
  unnest_tokens(word, headline)

#tokenized_h[,34]
```

## Transform headline words 

```{r}
#remove stop words 
tokenized_h <- tokenized_h %>%
  anti_join(stop_words)

#stem tribe words 
clean_tokens <- str_replace_all(tokenized_h$word,"land[a-z,A-Z]*","land") 

#remove numbers 
clean_tokens <- str_remove_all(clean_tokens, "[:digit:]")

clean_tokens <- gsub("’s", '', clean_tokens)

tokenized_h$clean <- clean_tokens

#remove the empty strings
tib <-subset(tokenized_h, clean!="")

#reassign
tokenized_h <- tib
```

## 4. Visualize common words based on headlines 
```{r}
tokenized_h %>%
  count(clean, sort = TRUE) %>%
  filter(n > 5) %>% 
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL) + 
  theme_classic() + 
  labs(y = "Most common words in headlines",
       x = "Word Frequency")

# ggsave(filename = "words_headlines.jpg", 
#        width = 4, 
#        height = 3, 
#        units = c("in"), 
#        dpi = 300)
```

# 4. Comparing word frequencies of NY Times articles' first paragraphs and headlines



# Assignment (Due by Week 3)

1.  Create a free New York Times account (<https://developer.nytimes.com/get-started>)

2.  Pick an interesting environmental key word(s) and use the jsonlite package to query the API. Pick something high profile enough and over a large enough time frame that your query yields enough articles for an interesting examination.

3.  Recreate the publications per day and word frequency plots using the first paragraph

-   Make some (at least 3) transformations to the corpus (add stopword(s), stem a key term and its variants, remove numbers)

4.  Recreate the publications per day and word frequency plots using the headlines variable (response.docs.headline.main). Compare the distributions of word frequencies between the first paragraph and headlines. Do you see any difference?


#knit to pdf to submit 