---
title: "hw4"
author: "Mia Forsline"
date: '2022-04-23'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE)

#install packages if necessary, then load libraries
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

librarian::shelf(
  here,
  lubridate,
  quanteda.sentiment,
  quanteda.textstats,
  reshape2,
  sentimentr,
  tidyr, #text analysis in R
  tidytext,
  tidyverse,
  wordcloud
  )
```

Read in the Tweet data 

```{r tweet_data}
raw_tweets <- read.csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv", header=TRUE)

dat <- raw_tweets[,c(5,7)] # Extract Date and Title fields (title = full text)
#dim(dat) = 10% sample of all the tweets that fit the time frame + topic 

tweets <- tibble(text = dat$Title,
                  id = seq(1:length(dat$Title)),
                 date = as.Date(dat$Date,'%m/%d/%y'))


#head(tweets$text, n = 10)
```

# 1. Think about how to further clean a twitter data set. Let’s assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

Clean up the Tweet data 

```{r cleaning_tweets}
#let's clean up the URLs from the tweets
tweets$text <- gsub("http[^[:space:]]*", "",tweets$text) #pull out https and replace them with nothing

tweets$text <- gsub("@*", "",tweets$text) #pull out @ symbol to remove mentions 

tweets$text <- str_to_lower(tweets$text) #convert text to lowercase 

#load sentiment lexicons
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')
```

Tokenize the Tweets into individual words 
```{r cleaning_tweets}
#tokenize tweets to individual words
#break it down to one word per row to pull out stop words and identify sentiment words 
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  left_join(bing_sent, by = "word") %>%
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")
```

# 2. Compare the ten most common terms in the tweets per day. Do you notice anything interesting?

- identify the top 10 most common words in all tweets for the entire time period
- graph 

```{r}
common_words <- words %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 10)

common_words_string <- common_words$word

common_words_over_time <- words %>% 
  subset(word %in% c(common_words_string))

common_words_count <- common_words_over_time %>% 
  group_by(date) %>% 
  count(word)

ggplot(data = common_words_count, aes(x = date, y = n, color = word)) +
  geom_line() + 
  theme_classic()
```

# 3. Adjust the wordcloud in the “wordcloud” chunk by coloring the positive and negative words so they are identifiable.

Comparison word cloud 
```{r wordcloud_comp}

words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "blue"),
                   max.words = 100)

#top black words = negative words
#bottom grey words = positive words 
```

# 4. Let’s say we are interested in the most prominent entities in the Twitter discussion. Which are the top 10 most tagged accounts in the data set. Hint: the “explore_hashtags” chunk is a good starting point.

- use quanteda 

```{r create_corpus}
corpus <- corpus(dat$Title) #enter quanteda
#summary(corpus)
#corpus = collection of documents, text entities, associated metadata 
#Ex. a corpus can contain a collection of tweets and their ID numbers 

#types = specific words - can have repeated multiple types of words 
#tokens = total words 

tokens <- tokens(corpus) #tokenize the text so each doc (page, in this case) is a list of tokens (words)

#examine the uncleaned version
#tokens

#tokens = words 
#column of lists bound together 

#clean it up
tokens <- tokens(tokens, remove_punct = TRUE, #remove punctuation
                      remove_numbers = TRUE) #remove numbers 

tokens <- tokens_select(tokens, stopwords('english'),selection='remove') #stopwords lexicon built in to quanteda

#tokens <- tokens_wordstem(tokens) #stem words down to their base form for comparisons across tense and quantity

tokens <- tokens_tolower(tokens)

#tokens
```

We can use the kwic function (keywords-in-context) to briefly examine the context in which certain words or patterns appear.

```{r initial_analysis}
head(kwic(tokens, pattern = "climate", window = 3))

head(kwic(tokens, pattern = phrase("climate change"), window = 3))

#we can view the words preceding/following the key word 

#twitter data limitations: sarcasm 

```

Hashtags in tweets 

```{r explore_hashtags}
#tokenize but keep only a particular pattern (in this case, the hashtag)
mention_tweets <- tokens(corpus, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "@*") #hashtag followed by any other string 

dfm_mention <- dfm(mention_tweets) #shows location of each tweet in the corpus (document feature matrix object)
#features = words 

tstat_freq <- textstat_frequency(dfm_mention, n = 10) 
head(tstat_freq, n = 10)
#freq = how many times the hashtag occurred
#docfreq = how many documents/tweets the hashtag occurred 
```

Create the sparse matrix representation known as the document-feature matrix. quanteda's textstat_polarity function has multiple ways to combine polarity to a single score. The sent_logit value to fun argument is the log of (pos/neg) counts.

```{r}

dfm <- dfm(tokens) #document feature matrix of all words

topfeatures(dfm, 12) #what are the most freq words? 

dfm.sentiment <- dfm_lookup(dfm, dictionary = data_dictionary_LSD2015) #quanteda.sentiment package
#dfm.sentiment
#features = emotion tags
#neg_positive ("not happy")
#neg_neg (2 negatives cancel out) 

head(textstat_polarity(tokens, data_dictionary_LSD2015, fun = sent_logit))
#calculate sentiment in a more detailed way = logit of the count = continuous scale rather than binary 

```


# 5. The Twitter data download comes with a variable called “Sentiment” that must be calculated by Brandwatch. Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch’s (hint: you’ll need to revisit the “raw_tweets” data frame).

```{r}
raw_tweets <- read.csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv", header=TRUE)

dat <- raw_tweets[,c(5,7)] # Extract Date and Title fields (title = full text)
#dim(dat) = 10% sample of all the tweets that fit the time frame + topic 

tweets <- tibble(text = dat$Title,
                  id = seq(1:length(dat$Title)),
                 date = as.Date(dat$Date,'%m/%d/%y'))

#list of characters
mytext <- get_sentences(tweets$text)

#sentiments from the text 
sent <- sentiment(mytext) %>% 
  group_by(element_id) %>% 
  summarise(avg_sentiment = mean(sentiment))

#create dataframe of sentiments of each sentence 
sentiment_df <- inner_join(raw_tweets, sent, by = "element_id")

sentiment <- sentiment_by(sentiment_df$headline)
```

