---
title: 'Topic 7: Word Embeddings'
output: html_document
---

```{r packages}
library(here)
library(tidytext)
library(tidyverse)
library(widyr)
library(irlba) #singluar value decomposition
library(broom) # creating search_synonym function
library(textdata)
library(ggplot2)
library(dplyr)

#https://semantle.com/
```

```{r data}
incidents_df<-read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/825b159b6da4c7040ce8295b9eae2fbbe9991ffd/dat/climbing_report_text.csv")
```

First, let's calculate the unigram probabilities, how often we see each word in this corpus.

```{r unigrams}
unigram_probs <- incidents_df %>%
    unnest_tokens(word, Text) %>%
    anti_join(stop_words, by = 'word') %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n)) 
unigram_probs 
```

Next, we need to know how often we find each word near each other word -- the skipgram probabilities. This is where we use the sliding window.

- slide a window across 
- Ex. 5 words that occur together 

```{r}
#window size of 5 
#nearly 3M windows 
skipgrams <- incidents_df %>%
    unnest_tokens(ngram, Text, token = "ngrams", n = 5) %>%
    mutate(ngramID = row_number()) %>% 
    tidyr::unite(skipgramID, ID, ngramID) %>%
    unnest_tokens(word, ngram) %>%
    anti_join(stop_words, by = 'word')

skipgrams
```

```{r}
#calculate probabilities 
#for each pair of words, how often did they appear together in a window ? 
#what is the probability that words occur in a window ? 
skipgram_probs <- skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))
```

Having all the skipgram windows lets us calculate how often words occur on their own, and how often words occur together with other words. We do this using the point-wise mutual information (PMI). It's the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. PMI tells us which words occur together more often than expected based on how often they occurred on their own.

```{r norm-prob}
#we need to also account for how frequent each word is overall - point-wise mutual information --> we need to normalize for the baseline frequency 

#normalize probabilities
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)
```


```{r norm-prob}
#Which words are most associated with "rope"?   
normalized_prob %>% 
    filter(word1 == "rope") %>%
    arrange(-p_together)

#what words co-occur with rope to a significant degree beyond their baseline frequencies
```

Now we convert to a matrix so we can use matrix factorization and reduce the dimensionality of the data.

```{r pmi}
#instead of a column for each pair of words, we want to reduce it to only 100 columns 
#reduce the dimensions of the matrix while retaining the maximum amount of info about how they're related 

pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)    
 
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

#run SVD using irlba() which is good for sparse matrices
pmi_svd <- irlba(pmi_matrix, 100, maxit = 500) #Reducing to 100 dimensions

#next we output the word vectors:
word_vectors <- pmi_svd$u #pull out the word vectors from one of the 3 decomposed matrices 
rownames(word_vectors) <- rownames(pmi_matrix)
```

```{r syn-function}
#take a single word from the word_vectors then compares it to the entire matrix, then outputs a similarity score
search_synonyms <- function(word_vectors, selected_vector) {
dat <- word_vectors %*% selected_vector
    
similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])

similarities %>%
       arrange(-similarity) %>%
        select(c(2,3))
}
```

```{r find-synonyms}
#try the function on the words fall and slip 
#feed into the function 
fall <- search_synonyms(word_vectors,word_vectors["fall",])
#fall
slip <- search_synonyms(word_vectors,word_vectors["slip",])
slip
```

```{r plot-synonyms}
slip %>%
    mutate(selected = "slip") %>%
    bind_rows(fall %>%
                  mutate(selected = "fall")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme(strip.text=element_text(hjust=0, size=12)) +
    scale_y_continuous(expand = c(0,0)) +
    labs(x = NULL, title = "What word vectors are most similar to slip or fall?")

#how similar are the vectors ? 
         
```

```{r word-math}
snow_danger <- word_vectors["snow",] + word_vectors["danger",] 
search_synonyms(word_vectors, snow_danger)

no_snow_danger <- word_vectors["danger",] - word_vectors["snow",] 
search_synonyms(word_vectors, no_snow_danger)
```

# Assignment: Download a set of pretrained vectors, GloVe, and explore them. 
Data: 
download.file('https://nlp.stanford.edu/data/glove.6B.zip',destfile = 'glove.6B.zip') 
unzip('glove.6B.zip') #Use this file: 'glove.6B.300d.txt'

# 1. Recreate the analyses in the last three chunks (find-synonyms, plot-synonyms, word-math) with the GloVe embeddings. How are they different from the embeddings created from the climbing accident data? Why do you think they are different?

# 2. Run the classic word math equation, "king" - "man" = ?

# 3. Think of three new word math equations. They can involve any words you'd like, whatever catches you interest.
