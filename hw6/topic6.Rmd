---
title: "Topic 6: Topic Analysis"
author: "Mia Forsline"
date: '2022-05-04'
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE)

#install packages if necessary, then load libraries
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

librarian::shelf(
  here,
  pdftools,
  quanteda,
  tm,
  topicmodels,
  ldatuning,
  tidyverse,
  tidytext,
  reshape2
  )

```

Load the data

```{r data}
##Topic 6 .Rmd here:https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/topic_6.Rmd

#grab data here: (81 comment documents)
comments_df<-read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/comments_df.csv")

#comments_df <- read_csv(here("dat", "comments_df.csv")) #if reading from local
```

Now we'll build and clean the corpus

```{r corpus}
epa_corp <- corpus(x = comments_df, text_field = "text")

epa_corp.stats <- summary(epa_corp)
head(epa_corp.stats, n = 25)

#some of the shortest documents will not be used in the model because the data is too sparse 
```


```{r corpus}
toks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)

#I added some project-specific stop words here that are not meaningful and don't tell us much 
add_stops <- c(stopwords("en"),"environmental", "justice", "ej", "epa", "public", "comment")
toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")

```

And now convert to a document-feature matrix

```{r dfm}
dfm_comm<- dfm(toks1, tolower = TRUE)
dfm <- dfm_wordstem(dfm_comm) #reduce words to their base forms 
dfm <- dfm_trim(dfm, min_docfreq = 2) #min number of documents the term must appear in 
#remove terms only appearing in one doc (min_termfreq = 10) #or you can specify the number of times you want the term to appear 

print(head(dfm)) #each row must have a non-zero value somewhere 
```


```{r dfm}
#remove rows (docs) with all zeros
sel_idx <- slam::row_sums(dfm) > 0 
dfm <- dfm[sel_idx, ]
#comments_df <- dfm[sel_idx, ]

```

We somehow have to come up with a value for k,the number of latent topics present in the data. How do we do this? There are multiple methods. Let's use what we already know about the data to inform a prediction. The EPA has 9 priority areas: Rulemaking, Permitting, Compliance and Enforcement, Science, States and Local Governments, Federal Agencies, Community-based Work, Tribes and Indigenous People, National Measures. Maybe the comments correspond to those areas?

```{r LDA_modeling}
#we need to tell the model how many topics there are 
k <- 9 

topicModel_k9 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))
#nTerms(dfm_comm) 

tmResult <- posterior(topicModel_k9)
attributes(tmResult)
#nTerms(dfm_comm)
```


```{r LDA_modeling}
#beta matrix 
beta <- tmResult$terms   # get beta from results
dim(beta)  # K distributions over nTerms(DTM) terms# lengthOfVocab
#tells us there are 9 topics 
```


```{r LDA_modeling}
terms(topicModel_k9, 10)
#tells us the 9 topics + top 10 words 
#each topic has a probability associated with each word 
```

Some of those topics seem related to the cross-cutting and additional topics identified in the EPA's response to the public comments:

1\. Title VI of the Civil Rights Act of 1964

2.[EJSCREEN](https://www.epa.gov/ejscreen/download-ejscreen-data)

3\. climate change, climate adaptation and promoting greenhouse gas reductions co-benefits

4\. overburdened communities and other stakeholders to meaningfully, effectively, and transparently participate in aspects of EJ 2020, as well as other agency processes

5\. utilize multiple Federal Advisory Committees to better obtain outside environmental justice perspectives

6\. environmental justice and area-specific training to EPA staff

7\. air quality issues in overburdened communities

So we could guess that there might be a 16 topics (9 priority + 7 additional based on public comments). Or we could calculate some metrics from the data.

```{r LDA_again}
#runs a series of model starting with a 2-topic model up to  a 20-topic model 
result <- FindTopicsNumber(
  dfm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"), #publication where these methods were first describrf
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)

FindTopicsNumber_plot(result)
#we can run up to 4 metrics simultaneously 
#we start with 2 topics in the model 
#top graph = the most topics we add, the better the model does (minimize the values)
#lower graph = more complicated (we want to maximize the values)
```


```{r LDA_again}
k <- 7

topicModel_k7 <- LDA(dfm, k, method="Gibbs", control=list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k7)
terms(topicModel_k7, 10)
```


```{r LDA_again}
theta <- tmResult$topics
beta <- tmResult$terms
vocab <- (colnames(beta))

```

There are multiple proposed methods for how to measure the best k value. You can [go down the rabbit hole here](#multiple%20proposed%20methods%20for%20measuring%20the%20best%20k%20value.%20You%20can)

```{r top_terms_topic}

comment_topics <- tidy(topicModel_k7, matrix = "beta")

top_terms <- comment_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
#beta value = the probability that the term is in that particular topic 
```

```{r plot_top_terms}

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

Let's assign names to the topics so we know what we are working with. We can name them by their top terms

```{r topic_names}
top5termsPerTopic <- terms(topicModel_k7, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")

#name each topic based on the first five words 
```

We can explore the theta matrix, which contains the distribution of each topic over each document

```{r topic_dists}
exampleIds <- c(1, 2, 3)
N <- length(exampleIds)

#lapply(epa_corp[exampleIds], as.character) #uncomment to view example text
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]

colnames(topicProportionExamples) <- topicNames

vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document=factor(1:N)), variable.name = "topic", id.vars = "document")  

ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)

#we can examine how the 9 topics are distributed over each document 
```

Here's a neat JSON-based model visualizer

```{r LDAvis}
library(LDAvis)
library("tsne")

svd_tsne <- function(x) tsne(svd(x)$u)
json <- createJSON(
  phi = tmResult$terms, 
  theta = tmResult$topics, 
  doc.length = rowSums(dfm), 
  vocab = colnames(dfm), 
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="", ylab="")
)
serVis(json)

#more data = more topics - the topics might overlap
#are overlapping topics distinct enough to remain separate ? or are they similar enough that they can be combined to reduce the overall number of topics ? 
```

### Assignment:

Either:

A) continue on with the analysis we started:

Run three more models and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis

OR

B) use the data you plan to use for your final project:

Prepare the data so that it can be analyzed in the topicmodels package

Run three more models and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis
